# Text-analysis
Given a list of websites containing content, I used beautifulSoup to scrape through the list of websites in order to extract the necessary information from each of the websites and store it in an extracted folder. From this folder we extract the data from each website and analyse it using the NLTK library. Detailed steps with explanation can be found in the code.  


[Instructions]  

[1] Ensure the the imported packages are all installed and available in the system.  
[2] The locations given in the code belong to the location in my laptop and cannot be used in other systems and will need to be modified accordingly in order  for the code to run correctly.  
[3] The extracted folder consists of the files that have content that has been extracted through the web scraping process.  
[4] Some of the URL’s show up with the Error 404 and have been avoided in the final output [Highlighted Red]  
[5] Few of the URL’s use different nomenclature when it comes to their class names and content id’s and have been handled accordingly.  
[6] The master Directory and StopWords directory are given as they were, but StopWords directory has a new file called ‘stop words.txt’ which combines all the other StopWords files into one file and is used in the program.  
[7] The entire code and explanation of each line can be found in analysis.ipynb and is done using Jupyter Notebook and should be executed in the given order itself.  
[8] The final output is stored in the given format in an Excel file named final_output.xlsx and the output.csv file is only used as a structural reference for the final_output file.  
